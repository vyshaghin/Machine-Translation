# Machine-Translation

The introduction of attention mechanism and its subsequent enhancement in the "Attention is All You Need" paper has transformed the NLP landscape. In this notebook, we will implement a custom encoder-decoder with attention mechanism architecture to solve the problem of machine translation.

### Objective

Translate Italian sequences to English sequences by implementing a attention mechanism based Seq2Seq model using custom layers.

### Requirements

numpy

pandas

tensorflow

keras

matplotlib

seaborn

python >= 3.7

Check the notebooks for more details

### Authors

Vyshagh A



